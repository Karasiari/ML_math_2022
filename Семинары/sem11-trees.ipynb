{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решающие деревья\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Murcha1990/ML_math_2022/blob/main/Семинары/sem11-trees.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это логический алгоритм классификации, решающий задачи классификации и регрессии. Данный алгоритм представляет собой объединение логических условий в структуру дерева.\n",
    "\n",
    "В зависимости от пространственной структуры данных, один типов моделей из них будет работать лучше:\n",
    "\n",
    "* Если данные хорошо линейно разделимы,  то линейная модель;\n",
    "* Если данные линейно неразделимы (присутствуют только кусочно-линейные или нелинейные зависимости), то решающие деревья.\n",
    "\n",
    "\n",
    "### План семинара:\n",
    "\n",
    "1. [Задачи](#tasks)\n",
    "2. [Дерево решений](#decisiontree)\n",
    "3. [Переобучение](#overfitting)\n",
    "4. [Неустойчивость](#unstable)\n",
    "5. [Пример на датасете](#dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задачи <a name=\"tasks\"></a>\n",
    "\n",
    "\n",
    "**Задача 1**. Постройте регрессионное дерево для прогнозирования $y$ с помощью $x$ на обучающей выборке:\n",
    "\n",
    "||||||\n",
    "|---|---|---|---|:---:|\n",
    "|$x_i$|0|1|2|3|\n",
    "|$y_i$|5|6|4|100|\n",
    "\n",
    "\n",
    "Критерий деления узла на два — минимизация $RSS$. Дерево строится до трёх терминальных узлов.\n",
    "\n",
    "\n",
    "**Задача 2**. Постройте регрессионное дерево для прогнозирования $y$ с помощью $x$ на обучающей выборке:\n",
    "\n",
    "    \n",
    "|$$y_{i}$$| $$x_{i}$$ |\n",
    "|:---:|:---:|\n",
    "|100|1|\n",
    "|102|2|\n",
    "|103|3|\n",
    "|50|4|\n",
    "|55|5|\n",
    "|61|6|\n",
    "|70|7|\n",
    "\n",
    "Критерий деления узла на два — минимизация $RSS$. Узлы делятся до тех пор, пока в узле остаётся больше двух наблюдений.\n",
    "\n",
    "\n",
    "**Задача 3**.\n",
    "\n",
    "Дон-Жуан предпочитает брюнеток. Перед Новым Годом он посчитал, что в записной книжке у него 20 блондинок, 40 брюнеток, две рыжих и восемь шатенок. С Нового Года Дон-Жуан решил перенести все сведения в две записные книжки, в одну — брюнеток, во вторую — остальных.\n",
    "\n",
    "Как изменились индекс Джини и энтропия в результате такого разбиения?\n",
    "\n",
    "\n",
    "\n",
    "**Задача 4**.\n",
    "\n",
    "Приведите примеры наборов данных, для которых индекс Джини равен $0$, $0.5$ и $0.999$.\n",
    "\n",
    "**Задача 5**.\n",
    "\n",
    "Машка пять дней подряд гадала на ромашке, а затем выкладывала очередную фотку «Машка с ромашкой» в инстаграмчик. Результат гадания — переменная $y_i$, количество лайков у фотки — переменная $x_i$. Постройте классификационное дерево для прогнозирования $y_i$ с помощью $x_i$ на обучающей выборке:\n",
    "\n",
    "|$$y_{i}$$| $$x_{i}$$ |\n",
    "|:---:|:---:|\n",
    "|плюнет|10|\n",
    "|поцелует|11|\n",
    "|поцелует|12|\n",
    "|к сердцу прижмет|13|\n",
    "|к сердцу прижмет|14|\n",
    "\n",
    "Дерево строится до идеальной классификации. Критерий деления узла на два — максимальное падение индекса Джини."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 6.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дерево решений  <a name=\"decisiontree\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сгенерируем пример линейно разделимой выборки для задачи бинарной классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 400\n",
    "np.random.seed(1)\n",
    "X = np.zeros((n, 2))\n",
    "X[:, 0] = np.linspace(-5, 5, n)\n",
    "X[:, 1] = X[:, 0] + 0.5 * np.random.normal(size=n)\n",
    "y = (X[:, 1] > X[:, 0]).astype(int)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=100, c=y, cmap='bwr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства, напишем функцию для обучения классификатора и построения графика с разделяющей прямой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model=LogisticRegression()):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=13)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    plot_decision_regions(X_test, y_test, model)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_score(y_pred, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним качество, так ли хорошо получится решить эту задачу не логитической регрессией, а деревом решений на линейно разделимой выборке?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(DecisionTreeClassifier(random_state=13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте вспомним, что такое логическое исключающее ИЛИ, и сгенерируем пример выборки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(n, 2)\n",
    "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0).astype(int)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=100, c=y, cmap=\"bwr\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переобучение <a name=\"overfitting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решающие деревья могут переобучаться под любую выборку, если их не регуляризовать: при большом количестве листьев для каждого объекта может выделиться своя область в признаковом пространстве. Дерево просто выучивает обучающую выборку, но не выделяет закономерности в данных. Давайте убедимся в этом на практике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "n = 100\n",
    "X = np.random.normal(size=(n, 2))\n",
    "X[:50, :] += 0.25\n",
    "X[50:, :] -= 0.25\n",
    "y = np.array([1] * 50 + [0] * 50)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=100, c=y, cmap=\"bwr\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, как разные значения гиперпараметров решающего дерева влияют на его структуру:\n",
    "\n",
    "- `max_depth`: максимальная глубина дерева\n",
    "- `min_samples_leaf`: минимальное число объектов в вершине дерева, необходимое для того, чтобы она стала листом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(15, 12))\n",
    "\n",
    "for i, max_depth in enumerate([3, 5, None]):\n",
    "    for j, min_samples_leaf in enumerate([1, 5, 15]):\n",
    "        dt = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, random_state=13)\n",
    "        dt.fit(X, y)\n",
    "        ax[i][j].set_title(\"max_depth = {} | min_samples_leaf = {}\".format(max_depth, min_samples_leaf))\n",
    "        ax[i][j].axis(\"off\")\n",
    "        plot_decision_regions(X, y, dt, ax=ax[i][j])\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На любой выборке (исключая те, где есть объекты с одинаковыми значениями признаков, но разными ответами) можно получить нулевую ошибку - с помощью максимально переобученного дерева:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=None, min_samples_leaf=1, random_state=13)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y, dt.predict(X)):.2f}\")\n",
    "\n",
    "plot_decision_regions(X, y, model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Неустойчивость  <a name=\"unstable\"></a>\n",
    "\n",
    "Как будет меняться структура дерева, если брать для обучения разные 90%-ые подвыборки из исходной?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(15, 12))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        seed_idx = 3 * i + j\n",
    "        np.random.seed(seed_idx)\n",
    "        dt = DecisionTreeClassifier(random_state=13)\n",
    "        idx_part = np.random.choice(len(X), replace=False, size=int(0.9 * len(X)))\n",
    "        X_part, y_part = X[idx_part, :], y[idx_part]\n",
    "        dt.fit(X_part, y_part)\n",
    "        ax[i][j].set_title(\"sample #{}\".format(seed_idx))\n",
    "        ax[i][j].axis(\"off\")\n",
    "        plot_decision_regions(X_part, y_part, dt, ax=ax[i][j])\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасет  <a name=\"dataset\"></a>\n",
    "\n",
    "Построим решающее дерево для какого-нибудь набора данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_california_housing()\n",
    "print(dataset[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=dataset[\"data\"], columns=dataset[\"feature_names\"])\n",
    "y = dataset[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=13)\n",
    "\n",
    "print(f\"Shape: {X.shape}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_depth=3, random_state=13)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "plot_tree(tree, feature_names=X.columns, filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(mean_squared_error(y_test, tree.predict(X_test)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_array = range(2, 20)\n",
    "mse_array = []\n",
    "\n",
    "for max_depth in max_depth_array:\n",
    "    tree = DecisionTreeRegressor(max_depth=max_depth, random_state=13)\n",
    "    tree.fit(X_train, y_train)\n",
    "    mse_array.append(mean_squared_error(y_test, tree.predict(X_test)))\n",
    "\n",
    "plt.plot(max_depth_array, mse_array)\n",
    "plt.title(\"Dependence of MSE on max depth\")\n",
    "plt.xlabel(\"max depth\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"max_depth\": max_depth_array,\"MSE\": mse_array}).sort_values(by=\"MSE\").reset_index(drop=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf_array = range(1, 20)\n",
    "mse_array = []\n",
    "\n",
    "for min_samples_leaf in min_samples_leaf_array:\n",
    "    dt = DecisionTreeRegressor(max_depth=6, min_samples_leaf=min_samples_leaf, random_state=13)\n",
    "    dt.fit(X_train, y_train)\n",
    "    mse_array.append(mean_squared_error(y_test, dt.predict(X_test)))\n",
    "    \n",
    "plt.plot(min_samples_leaf_array, mse_array)\n",
    "plt.title(\"Dependence of MSE on min samples leaf\")\n",
    "plt.xlabel(\"min samples leaf\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"min_samples_leaf\": min_samples_leaf_array,\"MSE\": mse_array}).sort_values(by=\"MSE\").reset_index(drop=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонус: решающее дерево своими руками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_m$ - множество объектов в разбиваемой вершине, $j$ - номер признака, по которому происходит разбиение, $t$ - порог разбиения.\n",
    "\n",
    "Критерий ошибки:\n",
    "\n",
    "$$\n",
    "Q(R_m, j, t) = \\frac{|R_\\ell|}{|R_m|}H(R_\\ell) + \\frac{|R_r|}{|R_m|}H(R_r) \\to \\min_{j, t}\n",
    "$$\n",
    "\n",
    "$R_\\ell$ - множество объектов в левом поддереве, $R_r$ - множество объектов в правом поддереве.\n",
    "\n",
    "$H(R)$ - критерий информативности, с помощью которого можно оценить качество распределения целевой переменной среди объектов множества $R$.\n",
    "\n",
    "_Реализуйте подсчет критерия ошибки. Для этого реализуйте функции для подсчета значения критерия информативности, а также для разбиения вершины._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_california_housing()\n",
    "X = pd.DataFrame(data=dataset[\"data\"], columns=dataset[\"feature_names\"])\n",
    "X[\"target\"] = dataset[\"target\"]\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.25, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "def H(R: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Compute impurity criterion for a fixed set of objects R.\n",
    "    Last column is assumed to contain target value\n",
    "    \"\"\"\n",
    "    # YOUR CODE\n",
    "\n",
    "\n",
    "def split_node(R_m: np.ndarray, feature: str, t: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split a fixed set of objects R_m given feature number and threshold t\n",
    "    \"\"\"\n",
    "    # YOUR CODE\n",
    "\n",
    "\n",
    "def q_error(R_m: np.ndarray, feature: str, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute error criterion for given split parameters\n",
    "    \"\"\"\n",
    "    # YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Переберите все возможные разбиения выборки по одному из признаков и постройте график критерия ошибки в зависимости от значения порога._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"MedInc\"\n",
    "Q_array = []\n",
    "\n",
    "feature_values = np.unique(X_train[feature])\n",
    "for t in feature_values:\n",
    "    Q_array.append(q_error(X_train, feature, t))\n",
    "\n",
    "plt.plot(feature_values, Q_array)\n",
    "plt.title(feature)\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"Q error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Напишите функцию, находящую оптимальное разбиение данной вершины по данному признаку._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_split(R_m: np.array, feature: str) -> Tuple[float, List[float]]:\n",
    "    # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, Q_array = get_optimal_split(X_train, feature)\n",
    "plt.plot(np.unique(X_train[feature]), Q_array)\n",
    "plt.title(feature)\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"Q error\")\n",
    "plt.axvline(x=t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Постройте графики критерия ошибки (в зависимости от количества объектов в левом поддереве) для каждого из признаков. Найдите признак, показывающий наилучшее качество. Какой это признак? Каков порог разбиения и значение качества? Постройте график критерия ошибки для данного признака в зависимости от значения порога._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for f in X_train.columns:\n",
    "    t, Q_array = get_optimal_split(X_train, f)\n",
    "    min_error = min(Q_array)\n",
    "    results.append((f, t, min_error))\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature: {} | optimal t: {} | min Q error: {:.2f}\".format(f, t, min_error))\n",
    "    plt.plot(np.unique(X_train[f]), Q_array)\n",
    "    plt.show()\n",
    "    \n",
    "results = sorted(results, key=lambda x: x[2])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results, columns=[\"feature\", \"optimal t\", \"min Q error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_feature, optimal_t, optimal_error = results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_feature)\n",
    "print(optimal_t)\n",
    "print(optimal_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Изобразите разбиение визуально. Для этого постройте диаграмму рассеяния целевой переменной в зависимости от значения найденного признака. Далее изобразите вертикальную линию, соответствующую порогу разбиения. Почему это разбиение может быть лучшим? Как вы можете интерпретировать результат?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[optimal_feature], y)\n",
    "plt.axvline(x=optimal_t, color=\"red\")\n",
    "plt.xlabel(optimal_feature)\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Feature: {} | optimal t: {} | Q error: {:.2f}\".format(optimal_feature, optimal_t, optimal_error))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонус\n",
    "\n",
    "Здесь можно посмотреть, как работает стрижка (cost-complexity pruning) для снижения переобучения деревьев: [ноутбук](https://colab.research.google.com/drive/1SGGaIGgo1wOsvBdPfiBb0128emHnu3Oo?usp=sharing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
