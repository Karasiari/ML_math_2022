{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "665acfac",
   "metadata": {},
   "source": [
    "# Семинар 8: Логистическая регрессия и SVM\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Murcha1990/ML_math_2022/blob/main/Семинары/sem08-logit_svm.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### План:\n",
    "1. Логистическая регрессия (напоминание)\n",
    "2. SVM (объяснение) + задачи.\n",
    "3. Ирисы Фишера. Свойства логистической регрессии и SVM\n",
    "4. Логистическая регрессия и SVM на менее приятных данных.\n",
    "5. ROC-кривая + задача. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что мы по-прежнему решаем задачу бинарной классификации, в которой целевая переменная $y$ принимает два значения: -1 и 1. \n",
    "\n",
    "На лекциях мы обсуждали, что эту идею можно удобно записать через функцию отступа:\n",
    "\n",
    "$$\n",
    "Q(X, w) = \\frac{1}{\\ell}\\sum_{i = 1}^{\\ell}[y_i \\langle x_i, w \\rangle < 0] = \\frac{1}{\\ell}\\sum_{i = 1}^{\\ell}[M_i < 0].\n",
    "$$\n",
    "\n",
    "Понятно, что такую функцию проблематично дифференцировать по $w$. Вместо этого будем минимизировать некоторую функцию $Q'(X, w)$, являющуюся верхней оценкой для $Q(X, w)$, и надеяться, что минимизация $Q'(X, w)$ позволит достаточно хорошо минимизировать и $Q(X, w)$.\n",
    "\n",
    "Логистическая регрессия предлагает использовать логистическую функцию потерь:\n",
    "\n",
    "$$\n",
    "Q'(X, w) = \\frac{1}{\\ell}\\sum_{i = 1}^{\\ell}\\log(1 + e^{-y_i \\langle x_i, w \\rangle}) \\rightarrow \\min_w.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((np.linspace(-3, 0, 500), np.linspace(0, 3, 500)))\n",
    "np.random.seed(123)\n",
    "y = np.ones(1000)\n",
    "w = np.ones(1000)\n",
    "M = y * x * w\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "_ = plt.plot(x * w, x < 0, label = '$Q(X, w)$, zero-one loss')\n",
    "_ = plt.plot(M, np.log2(1 + np.exp(-M)), label = \"$Q'(X, w)$, logistic loss\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для предсказания вероятностей используем **сигмоиду**:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\dfrac{1}{1 + e^{-z}}\\in[0;1].\n",
    "$$\n",
    "\n",
    "Подставляя вместо $z$ скалярное произведение $(w,x)$, получаем формулу для предсказания логистической регрессии:\n",
    "$$a(x) = \\dfrac{1}{1 + e^{-(w,x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Регуляризация** вводится таким же образом, как это было в случае линейной регрессии. Например, функция потерь для $l$-$2$ регуляризации выглядит так:\n",
    "\n",
    "$$\n",
    "\\bar{Q}'(X, w) = Q'(X, w) + \\frac{1}{2}\\lambda\\|w\\|^2_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод опорных векторов (SVM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод опорных векторов – математически строго обоснованный метод, идея которого состоит в максимизации ширины разделяющей полосы между классами.\n",
    "\n",
    "Мы по-прежнему решаем задачу бинарной классификации и используем классификатор $f(x_i, w) = \\mathrm{sign}(\\langle x_i, w\\rangle)$. Предположим, что мы работаем с линейно разделимой выборкой. Мы хотим максимизировать отступ (расстояние от точки до разделяющей поверхности) классификатора:\n",
    "\n",
    "$$\n",
    "\\rho(x_i, \\langle x, w\\rangle) =  \\min_i\\dfrac{|{\\langle x_i, w\\rangle|}}{\\|w\\|}.\n",
    "$$\n",
    "\n",
    "Воспользуемся картинкой из Википедии, чтобы лучше понять эту идею:\n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/1920px-SVM_margin.png\" alt=\"drawing\" width=\"400\"/>](https://en.wikipedia.org/wiki/Support-vector_machine#/media/File:SVM_margin.png)\n",
    "\n",
    "В линейно-разделимом случае задача SVM выглядит так:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\|w\\|^2 \\to \\min_{w}, \\\\\n",
    "y_i(\\langle x_i, w\\rangle) \\ge 1, \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Если выборка не является линейно-разделимой, то нам придётся позволить линейному классификатору допускать ошибки на некоторых наблюдениях. Тогда задача превращается в поиск оптимального выбора между максимизацией ширины разделяющей полосы и ошибок классификации:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\|w\\|^2 + C \\sum_{i = 1}^{\\ell} \\xi_i \\to \\min_{w, \\xi_i}, \\\\\n",
    "y_i(\\langle x_i, w\\rangle) \\ge 1 - \\xi_i, \\\\\n",
    "\\xi_i \\ge 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$C$ – параметр, который позволяет регулировать пропорции этого выбора. Чем больше $C$, тем больше штраф за неверную классификацию.\n",
    "\n",
    "![Задание 1](pic2.png) \n",
    "\n",
    "Путём хитрых математических преобразований можно показать, что итоговая функция потерь SVM выглядит следующим образом:\n",
    "\n",
    "$$\n",
    "Q(X, w) = \\sum_{i=1}^{l} \\max\\{0, 1 - y_i(\\langle x_i, w\\rangle)\\} + \\frac1C\\|w\\|^2 \\to \\min_w\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.**\n",
    "![Задание 1](https://raw.githubusercontent.com/Murcha1990/ML_math_2022/main/Семинары/images/SVM_task.png) \n",
    "\n",
    "**Задание 2.**\n",
    "По картинке качественно решите задачу классификации методом опорных векторов.\n",
    "\n",
    "* как пройдет разделяющая полоса при маленьком С?\n",
    "* как пройдет разделяющая полоса при большом С?\n",
    "\n",
    "![Задание 1](https://raw.githubusercontent.com/Murcha1990/ML_math_2022/main/Семинары/images/SVM_task2.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ирисы Фишера. Свойства логистической регрессии и SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим свойства логистической регрессии и метода опорных векторов на примере классического набора данных [\"Ирисы Фишера\"](https://ru.wikipedia.org/wiki/Ирисы_Фишера). Этот набор состоит из 150 наблюдений, каждое из которых представляет собой четыре измерения: длина наружной доли околоцветника (`sepal length`), ширина наружной доли околоцветника (`sepal width`), длина внутренней доли околоцветника (`petal length`), ширина внутренней доли околоцветника (`petal width`). Каждое наблюдение относится к одному из трёх классов ириса: `setosa`, `versicolor` или `virginica`. Задача состоит в том, чтобы по измерениям предсказать класс цветка. \n",
    "\n",
    "![](https://raw.githubusercontent.com/Murcha1990/ML_math_2022/main/Семинары/images/pic3.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data['data'], columns = data['feature_names'])\n",
    "y = data['target']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем работать с двумя признаками: `sepal length (cm)` и `sepal width (cm)`. Отделите их в отдельную матрицу. Разделим выборку на обучающую и тестовую, долю тестовой выборки укажите равной 0.3. Отмасштабируем выборки при помощи StandardScaler. Построим диаграмму рассеяния по тренировочной выборке и убедимся, что данные линейно не разделимы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "np.random.seed(123)\n",
    "\n",
    "X = X[['sepal length (cm)', 'sepal width (cm)']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с простого. Обучим логистическую регрессию и SVM с линейным ядром на тренировочной выборке и убедимся, что полученные оценки весов действительно различаются. Убедимся, что `accuracy`, возможно, не подходит в качестве метрики для данной задачи и рассчитаем `f1-меру` на тестовой выборке. Какой алгорим показал более высокое качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "lr = LogisticRegression(multi_class=\"ovr\") \n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "svm = SVC(kernel='linear', ) \n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print(lr.coef_)\n",
    "print(svm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим, как различаются решающие поверхности алгоритмов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код ниже построит решающие поверхности для классификаторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1797f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "labels = ['Logistic Regression', 'SVM']\n",
    "for clf, lab, grd in zip([lr, svm],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1], repeat=2)):\n",
    "\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X = X_train, y = np.array(y_train), clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series(y_train).value_counts())\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('LR: ', f1_score(y_test, lr.predict(X_test), average=\"micro\"))\n",
    "print('SVM: ', f1_score(y_test, svm.predict(X_test), average=\"micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь изучим свойства каждого классификатора по-отдельности. Начнём с логистической регрессии.\n",
    "\n",
    "Обучим три различные логистические регрессии с разным параметром регуляризации $\\alpha$ (обратите внимание, что в реализации `sklearn` $C = 1/\\alpha$). Как изменяется разделяющая поверхность в зависимости от $\\alpha$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "C = [0.01, 0.05, 10] # smaller = stronger reg\n",
    "\n",
    "lr1 = LogisticRegression(C = C[0])\n",
    "lr2 = LogisticRegression(C = C[1])\n",
    "lr3 = LogisticRegression(C = C[2])\n",
    "\n",
    "lr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "labels = ['C = 0.01', 'C = 0.05', 'C = 10']\n",
    "for clf, lab, grd in zip([lr1, lr2, lr3],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1, 2], repeat=2)):\n",
    "    clf.fit(X_train, y_train)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X = X_train, y = np.array(y_train), clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдём к SVM.\n",
    "\n",
    "Обучим три SVM с линейным ядром с разным параметром регуляризации $C$. Как изменяется разделяющая поверхность в зависимости от $C$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.04, 0.05, 1] # smaller = stronger reg\n",
    "\n",
    "svc1 = SVC(C = C[0], kernel = 'linear', decision_function_shape=\"ovr\")\n",
    "svc2 = SVC(C = C[1], kernel = 'linear', decision_function_shape=\"ovr\")\n",
    "svc3 = SVC(C = C[2], kernel = 'linear', decision_function_shape=\"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "labels = ['C = 0.04', 'C = 0.05', 'C = 1']\n",
    "for clf, lab, grd in zip([svc1, svc2, svc3],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1, 2], repeat=2)):\n",
    "    clf.fit(X_train, y_train)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X = X_train, y = np.array(y_train), clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия и SVM на менее приятных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с [набором данных](https://www.kaggle.com/piyushgoyal443/red-wine-dataset?select=wineQualityReds.csv), содержащим информацию о характеристиках вина. Каждое наблюдение принадлежит к одному из 10 категорий качества вина, и наша задача заключается в том, что предсказать эту категорию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/Murcha1990/ML_math_2022/main/Семинары/data/wineQualityReds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('wineQualityReds.csv', index_col = 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как указано в описании набора, в нём нет пропущенных значений, и все переменные являются непрерывными. Целевая переменная – `quality`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдём к задаче бинарной классификации и будем предсказывать только наиболее популярную категорию качества. Закодируйте столбец `quality` так, чтобы наиболее частая категория (это категория 5) получила метку 1, а все прочие категории – метку -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual = data.quality.copy()\n",
    "qual[qual != 5] = 0\n",
    "qual[qual == 5] = 1\n",
    "data['quality'] = qual\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим признаки и целевую переменную. Разделим выборку на тренировочную и тестовую, долю тестовой выборки укажем равной 0.3. При помощи `StandardScaler` отмасштабируем тренировочную и тестовую выборки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('quality', axis = 1), data['quality'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При помощи кросс-валидации подберем оптимальные значения коэффициентов регуляризации для логистической регрессии и SVM с линейным ядром. Обучим модели с этими параметрами. Убедимся, что доля правильных ответов – не лучший вариант для нашей задачи и рассчитаем F-меру на тестовой выборке. Какой алгоритм показал себя лучше? \n",
    "\n",
    " **Бонус:** качество работы SVM можно улучшить за счёт применения ядер, после чего разделяющая поверхность становится нелинейной. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores_lr = []\n",
    "scores_svm = []\n",
    "\n",
    "for c in np.arange(0.1, 10, 1):\n",
    "    lr = LogisticRegression(C = c)\n",
    "    svm = SVC(C = c)\n",
    "\n",
    "    cv_lr = cross_validate(lr, X_train, y_train, cv = 5, scoring=('f1'))['test_score']\n",
    "    cv_svm = cross_validate(svm, X_train, y_train, cv = 5, scoring=('f1'))['test_score']\n",
    "    scores_lr.append(cv_lr.mean())\n",
    "    scores_svm.append(cv_svm.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_lr, '\\n')\n",
    "print(scores_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(scores_lr), max(scores_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC-кривая\n",
    "\n",
    "Очень советуем почитать: https://dyakonov.org/2017/07/28/auc-roc-площадь-под-кривой-ошибок/\n",
    "\n",
    "Пример для понимания (из статьи выше):\n",
    "![Данные 1](https://raw.githubusercontent.com/Murcha1990/ML_math_2022/main/Семинары/images/table1.png) \n",
    "![ROC-кривая](https://raw.githubusercontent.com/Murcha1990/ML_math_2022/main/Семинары/images/pic1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы узнали, что помимо accuracy в задачах классификации так же используются precision, recall и f-мера. Теперь пришло время познакомиться с ещё одной метрикой – ROC AUC.\n",
    "\n",
    "Для начала вспомним, что мы работаем с матрицей ошибок:\n",
    "\n",
    "|       | alg = 1          | alg = -1    |\n",
    "|-------| -----------------|-------------|\n",
    "|y = 1  |TP                |FN           |\n",
    "|y = -1 |FP                | TN          |\n",
    "\n",
    "Определим следующие величины:\n",
    "\n",
    "$$\n",
    "TPR \\text{ (true positive rate, recall, sensitivity)} = \\dfrac{TP}{TP + FN} –\n",
    "$$\n",
    "доля правильно предсказанных объектов положительного класса.\n",
    "\n",
    "$$\n",
    "FPR \\text{ (false positive rate, 1 - specificity)} = \\dfrac{FP}{FP + TN} –\n",
    "$$\n",
    "доля неправильно предсказанных объектов отрицательного класса.\n",
    "\n",
    "Рассмотрим задачу мягкой классификации: мы предказываем вероятности принадлежности наблюдения к положительному и отрицательному классам. Тогда TPR и FPR будут зависеть от порога для вероятности, выше которого наблюдение будет отнесено к положительному классу. ROC-кривая строится в координатах $(FPR, TPR)$ и показывает комбинации TPR и FPR при всевозможных значениях порога. \n",
    "\n",
    "Для хорошего классификатора эта кривая является вогнутой, а для идеального классификатора она будет проходить через точку $(0, 1)$ (почему?).\n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png\" alt=\"drawing\" width=\"350\"/>](https://ru.wikipedia.org/wiki/ROC-кривая)\n",
    "\n",
    "\n",
    "\n",
    "**Задание 3.** Постройте ROC-кривую для следующей выборки (на листочке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels\n",
    "y = [-1, 1, 1, -1, 1, 1]\n",
    "# Predicted labels\n",
    "p = [0.5, 0.1, 0.2, 0.9, 0.7, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решение:**\n",
    "1. Упорядочим наблюдения по убыванию ответов алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [-1, 1, -1, 1, 1, 1]\n",
    "p = [0.9, 0.7, 0.5, 0.2, 0.1, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разобьём единичный квадрат на $(m, n)$ частей, где $m$ – число 1 в $y$, $n$ – число нулей. Стартуем из точки $(0, 0)$. Если значение $y$ равно 1, делаем шаг вверх, а если -1 – вправо. Понятно, что конечная точка нашего маршрута – точка $(1, 1)$.\n",
    "\n",
    "**Важный момент:** если у нескольких объектов значения предсказаний равны, а $y$ – различны, то мы должны сделать ход \"по диагонали\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Полученная кривая и является ROC-кривой. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Вычислим ROC AUC для построенной ROC-кривой. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В `sklearn` реализовано вычисление значений ROC-кривой и площади под ней. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим ROC-кривую и рассчитаем площадь под ней для логистической регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "tpr, fpr, _ = roc_curve(y_train, lr.predict_proba(X_train)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f20f5863f24dac35637d7f882b531b7edd6cd78621bb359b5b6d94a3de44fe0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
