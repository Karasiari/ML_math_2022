{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес, бустинг и bias-variance tradeoff\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Murcha1990/ML_math_2022/blob/main/Семинары/sem12-boosting.ipynb)\n",
    "\n",
    "### План семинара:\n",
    "\n",
    "1. Случайный лес\n",
    "2. Bias-variance decomposition\n",
    "3. Градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlxtend==0.21.0 matplotlib==3.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.data import iris_data\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес <a name=\"randomforest\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, из каких шагов складывается обучение случайного леса. Алгоритм из $ N$ деревьев, выглядит следующим образом:\n",
    "\n",
    "\n",
    "Для каждого $n = 1, \\dots, N$: <br>\n",
    "1. Сгенерировать выборку $ X_n$ с помощью бутстрапа; <br>\n",
    "2. Построить решающее дерево $ b_n$ по выборке $ X_n$: <br>\n",
    "— по заданному критерию выбираем лучший признак, делаем разбиение в дереве по нему и так до исчерпания выборки <br>\n",
    "— дерево строится, пока в каждом листе не более $ n_\\text{min}$ объектов или другому критерию останова <br>\n",
    "— при каждом разбиении сначала выбирается $ m$ случайных признаков из $ n$ исходных, \n",
    "и оптимальное разделение выборки ищется только среди них.\n",
    "\n",
    "Так, при построении каждого дерева при созданию нового узла случайным образом выбираем набор признаков для разбиения. Таким образом, мы снизим дисперсию итоговой модели, поскольку уменьшаем скоррелированность деревьев. \n",
    "\n",
    "Итоговый классификатор имеет вид $ a(x) = \\frac{1}{N}\\sum_{i = 1}^N b_i(x)$, где $b_i$ - одно из деревьев (базовый алгоритм). Для задачи кассификации мы выбираем класс через правило голосования большинства, а в задаче регрессии — берем средний прогноз.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем датасет для задачи многоклассовой классификации и обучим для ее решения случайный лес:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        rfc, X_train, y_train, X_test, y_test, \n",
    "        loss='0-1_loss',\n",
    "        random_seed=42)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сравнения, посчитаем усредненные смещение и дисперсию для более простого алгоритма (решающего дерева):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        tree, X_train, y_train, X_test, y_test, \n",
    "        loss='0-1_loss',\n",
    "        random_seed=42)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните ошибку, смещение и дисперсию двух данных алгоритмов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопросы на обсуждение**:\n",
    "1. Что такое out-of-bag ошибка?\n",
    "2. Может ли случайный лес переобучиться?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Out-of-bag-ошибка** - это усредненная ошибка на непопавших в выборку объектах по всему итоговому алгоритму.  \n",
    "\n",
    "Вспомним, что при обучении случайного леса каждое дерево строится на подвыборке, бустрапированной из исходной обучающей выборки. Соответственно, будут такие наблюдения, которые ни разу не попадут ни в одну из подвыборок. Данные объекты можно использовать для валидации при оценке качества алгоритма. Для этого, воспользуемся при инициализации модели параметром `oob_score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=50, oob_score=True, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "model.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Попробуем ответить на вопрос, используя датасет по предсказанию степени диабета (в диапазоне вещественных чисел 25 - 346) для задачи регрессии, построив случайный лес для разного количества деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse, test_mse = [], []\n",
    "n_trees = 50\n",
    "\n",
    "for i in tqdm(range(1, n_trees)):\n",
    "    model = RandomForestRegressor(n_estimators=i, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_mse.append(mean_squared_error(y_train, model.predict(X_train)))\n",
    "    test_mse.append(mean_squared_error(y_test, model.predict(X_test)))\n",
    "    \n",
    "plt.plot(train_mse, label='train')\n",
    "plt.plot(test_mse, label='test')\n",
    "plt.xlabel('number of trees')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Number of trees')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что графики напоминают геометрическое распределение, и что при определенном значении количества деревьев ошибка начинает убывать уже слабее. Какой вывод можно сделать с точки зрения переобучения при увеличении числа деревьев? \n",
    "\n",
    "Попробуйте провести эксперименты относительно переобучения случайного леса, поменяв другие параметры, например, `max_depth`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias - variance decomposition <a name=\"decompose\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка любой модели складывается из 3 факторов: сложности выборки, схожести обученной модели с истинной зависимостью с ответами в выборке и свойств семейства, к которому относится выбранная модель. Между этими факторами существует баланс, при котором увеличение одной составляющей приводит к уменьшению другой. \n",
    "\n",
    "Любую функцию ошибки можно разложить на три компоненты, описывающие сложность и качество построенного алгоритма: **смещение** (bias), **дисперсию** (variance) и **шум** (noise). \n",
    "\n",
    "Рассмотрим разложение на смещение и дисперсию на примере задачи регрессии. \n",
    "\n",
    "Пусть задана некоторая выборка $(x_i, y_i)$, при $i = 1, ..., N$, прогнозы для каждого i-го объекта обозначим как $\\hat y_i$. Рассмотрим уже известную нам квадратичную функцию потерь: $Q(a, y) = (y - \\hat y)^2$. Математическое ожидание среднеквадратичной ошибки можно представить как:\n",
    "\n",
    "$$\\mathbb{E}[y - \\hat{y}]^2 = bias^2 + variance + noise,$$\n",
    "\n",
    "где\n",
    "\n",
    "|||||\n",
    "|---|---|:---:|---|\n",
    "|смещение|$\\mathrm{bias}$|$\\mathbb{E}(\\hat{y}) - y$|ошибка из-за простоты модели (отклонение среднего прогноза модели от истинных ответов)|\n",
    "|дисперсия|$\\mathrm{variance}$|$\\mathbb{E}[\\mathbb{E}(\\hat{y}) - \\hat{y}]^2$| ошибка из-за сложности модели (разброс прогнозов модели относительно ее среднего прогноза|\n",
    "|шум|$\\mathrm{noise}$|$\\mathbb{E}[y - \\mathbb{E}(y)]^2$|ошибка идеальной модели (из-за наличия шума в данных)|\n",
    "\n",
    "\n",
    "Разбор разложения MSE на смещение и дисперсию в общем виде можно посмотреть в лекции или [тут](https://github.com/esokolov/ml-course-hse/blob/master/2020-fall/lecture-notes/lecture08-ensembles.pdf) в общем виде. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим пример решения задачи регресси на данных с наличием нелинейных связей - например, через сигмоиду, как предложено [здесь](https://dyakonov.org/2018/04/25/%D1%81%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D0%B5-bias-%D0%B8-%D1%80%D0%B0%D0%B7%D0%B1%D1%80%D0%BE%D1%81-variance-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82/comment-page-1/). \n",
    "\n",
    "Сгенерируем случайную выборку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 300\n",
    "X = np.linspace(-10, 10, N).reshape(-1, 1)\n",
    "y = np.sin(X) + np.random.normal(0, 0.5, N).reshape(-1, 1)\n",
    "plt.scatter(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на модели с разной сложностью и проанализируем их с точки зрения смещения и дисперсии. Построим модель линейной регрессии на признаке X, а потом попробуем усложнять эту модель через добавление новых полиноминальных признаков с разной степенью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_polinomial_reg(degree=1):\n",
    "    polynomial = PolynomialFeatures(degree)\n",
    "    new_features = polynomial.fit_transform(X)\n",
    "    polreg = LinearRegression()\n",
    "    polreg.fit(new_features, y)\n",
    "    preds = polreg.predict(new_features)\n",
    "    return preds\n",
    "\n",
    "# Линейная регрессия\n",
    "linreg = train_polinomial_reg(1)\n",
    "\n",
    "# Регрессия на полиноминальных признаках со степень 5\n",
    "polinoms_5 = train_polinomial_reg(5)\n",
    "\n",
    "# Регрессия на полиноминальных признаках со степень 15\n",
    "polinoms_15 = train_polinomial_reg(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X, linreg, label='Linear regression', c='g')\n",
    "plt.plot(X, polinoms_5, label='Linear regression + PolynomialFeatures(5)', c='y')\n",
    "plt.plot(X, polinoms_15, label='Linear regression + PolynomialFeatures(15)', c='r')\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно увидеть на графиках, простые модели недообучаются, сложные модели - переобучаются. Простая модель имеет высокую дисперсию и малое смещение, и в данном случая простая линейная регрессия (с полиномом первой степени) плохо подходит для описания целевой переменной. В то же время, более сложные модели по мере увеличения степени полинома имеет меньшую дисперсию, но большее смещение. \n",
    "\n",
    "\n",
    "|**model's complexity**|**bias**|**variance**\n",
    "|---|:---:|:---:|\n",
    "|simple|high|low\n",
    "|complex|low| high\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом, на обучающей выборке ошибка MSE будет, действительно, убывать по мере увеличения степени полинома:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mean_squared_error(x, y) for x in [linreg, polinoms_5, polinoms_15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но если мы поделим выборку на train и test, и будем оценивать ошибку на тесте, то увидим переобучение при возрастающей сложности алгоритма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 200 \n",
    "\n",
    "def get_preds_on_test(degree=1, train_size=200):\n",
    "    polynomial = PolynomialFeatures(degree)\n",
    "    new_features = polynomial.fit_transform(X)\n",
    "    polreg = LinearRegression()\n",
    "    X_train, y_train = new_features[:train_size], y[:train_size]\n",
    "    X_test, y_test = new_features[train_size:], y[train_size:]\n",
    "    polreg.fit(X_train, y_train)\n",
    "    preds = polreg.predict(X_test)\n",
    "    return preds\n",
    "\n",
    "linreg = get_preds_on_test(1, 200)\n",
    "polinoms_5 = get_preds_on_test(5, 200)\n",
    "polinoms_15 = get_preds_on_test(15, 200)\n",
    "y_test = y[train_size:]\n",
    "\n",
    "\n",
    "[mean_squared_error(x, y_test) for x in [linreg, polinoms_5, polinoms_15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://neerc.ifmo.ru/wiki/images/1/18/Bias-Variance-Tradeoff.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подводя небольшой итог - нам приходится искать компромисс между между простотой и сложностью модели, между недообучением и переобучением. С точки зрения смещения и дисперсии, обычно наилучшее качество достигается при средних значениях обоих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бустинг\n",
    "\n",
    "**Ключевая идея** градиентного бустинга - строим набор базовых алгоритмов, каждый из которых исправляет ошибку предыдущих. Так же, как и для дерева или случайного леса, можем использовать градиентый бустинг и для задачи регрессии, и для задачи классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задачи по теории  <a name=\"tasks\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дана выборка из четырех наблюдений. На ней построены два дерева:\n",
    "\n",
    "|$$x_{i}$$|$$y_{i}$$|\n",
    "|---|---|\n",
    "|1|6|\n",
    "|2|6|\n",
    "|3|12|\n",
    "|4|18|\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/Murcha1990/ML_math_2022/main/Семинары/images/task1.png' width=400>\n",
    "\n",
    "Давайте построим бэггинг. Первому дереву достались наблюдения 1, 1, 2 и 3. Второму дереву - наблюдения 2, 3, 4 и 4. \n",
    "\n",
    "**Задача 1.** Будем строить прогнозы в каждом листе, минимизируя сумму квадратов ошибок. \n",
    "\n",
    "1. Какие прогнозы внутри обучающей выборки мы получим с помощью такого леса?\n",
    "2. Сколько деревьев имеет смысл посадить, чтобы получить хорошие вневыборочные прогнозы по четырем наблюдениям?\n",
    "\n",
    "**Задача 2.** Теперь попробуем построить прогнозы, минимизируя функцию:\n",
    "\n",
    "$$Q = \\sum_{i=1}^n (y_i - \\hat y_i)^2 + \\lambda \\sum_{j=1}^T w_j^2,$$\n",
    "\n",
    "где $y_i$ — прогнозируемое значение для $i$-го наблюдения, $n$ — количество наблюдений, $w_j$ — прогноз в $j$-ом листе, $T$ — количество листов на дереве.\n",
    "\n",
    "1. Какие прогнозы внутри обучающей выборки мы получим с помощью такого леса? \n",
    "2. Построим бустинг с темпом обучения $\\mu$ и такой же функцией потерь. Какие прогнозы внутри обучающей выборки мы получим при $\\mu = 0.5$ и $\\lambda = 1$? \n",
    "3. Сколько деревьев для бэггинга имеет смысл посадить, чтобы получить хорошие вневыборочные прогнозы по четырем наблюдениям? Для бустинга?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Градиентный бустинг в sklearn  <a name=\"sklearn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Сегодня поработаем с датасетом `Customer Personality Analysis` с платформы Kaggle. Подробнее про данные можно прочитать [тут](https://www.kaggle.com/imakash3011/customer-personality-analysis).\n",
    "\n",
    "Набор данных содрежит доступную информацию по клиентам и их покупательскому поведению. Сбор таких данных помогает бизнесу лучше понимать своих клиентов и облегчает им изменение продуктов в соответствии с конкретными потребностями, поведением и проблемами разных типов клиентов.\n",
    "\n",
    "На основе социально-демографических признаков и интересных статистик по истории покупок продуктов и участию в предыдущих маркетинговых акциях попробуем для каждого клиента предсказать, участвовал ли он в последней маркетинговой акции или нет:\n",
    "\n",
    "`Response: 1 if customer accepted the offer in the last campaign, 0 otherwise`\n",
    "\n",
    "Эта задача часто используется в маркетинге, чтобы заранее предсказывать, насколько новая промоакция будет успешной для выбранного сегмента пользователей.\n",
    "\n",
    "Посмотрим на датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/Murcha1990/ML_math_2022/main/Семинары/data/marketing_campaign.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('marketing_campaign.csv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом шаге нужно отдельно провести EDA: посмотреть, сколько у нас наблюдений и признаков, какого типа данных каждый признак; проверить наличие пропусков, выбросов и обработать их; сделать препроцессинг признаков - например, закодировать и нормализовать их; можно сгенерировать новые признаки на основе исходных и сделать обзор признаков и другое.\n",
    "\n",
    "Все эти шаги очень важны, но т.к. в этот раз хотим сфокусироваться на ансамблях моделей, отберем только пару признаков из датасета, обработаем пропущенные значения, и перейдем к обучению моделей для их сравнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Year_Birth', 'Income', 'Kidhome', 'Recency', 'MntWines', \n",
    "        'MntSweetProducts', 'NumWebPurchases', 'NumStorePurchases', \n",
    "        'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', \n",
    "        'AcceptedCmp5']]\n",
    "y = df['Response']\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm(X.columns):\n",
    "    X[col] = X[col].fillna(np.mean(X[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся имплементацией градиентного бустинга из sklearn. Обратите внимание, доступны отдельно модели для классификации и регрессии - `GradientBoostingClassifier` и `GradientBoostingRegressor`.\n",
    "\n",
    "Посмотрим на часть наиболее важных параметров для инициализации модели градиентного бустинга (для классификации). Как можно заметить, очень похоже на случайный лес:\n",
    "\n",
    "```\n",
    "loss='deviance' - функция потерь,\n",
    "learning_rate=0.1 - скорость обучения,\n",
    "n_estimators=100 - количество деревьев,\n",
    "subsample=1.0 - доля наблюдений для обучения каждого базового алгоритма,\n",
    "criterion='friedman_mse' - информационный критерий,\n",
    "min_samples_split=2 - минимальное количество наблюдений, чтобы продолжать разбиение выборки в текущем узле,\n",
    "min_samples_leaf=1 - минимальное количество наблюдений в листе,\n",
    "max_depth=3 - максимальная глубина каждого базового алгоритма,\n",
    "min_impurity_decrease=0.0 - сплит узла будет сделан, только если удалось достигнуть большего или равного увеличения значения по метрике степени упорядоченности выборки (impurity)\n",
    "по формуле N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)\n",
    "min_impurity_split=None - то же самое, только задаем само значение метрики,\n",
    "max_features=None - максимальное количество признаков, используемое для сплита,\n",
    "verbose=0 - показывать ли в выводе ячейки каждую итерацию обучения\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для разминки, построим градиентный бустинг с 10 деревьями. Поменяем только критерий и выведем значения функции ошибки для каждой итерации, остальные параметры оставим дефолтными. Обратите внимание, что в задаче классификации композиция базовых алгоритмов будет выдавать вещественные числа, которые по смыслу являются оценками логита - логарифм отношения вероятности положительного класса к вероятности отрицательного класса, поэтому используем `mse`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(\n",
    "    criterion='mse',\n",
    "    n_estimators=10,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "round(cross_val_score(model, X, y, cv=3, scoring='f1').mean(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним, как ведут себя градиентный бустинг и случайный лес (частный случай бэггинга) с ростом числа базовых алгоритмов.\n",
    "\n",
    "В **случайном лесе** все деревья настраиваются на различные выборки из одного и того же распределения. При этом, некоторые из них могут переобучиться, но усреднение позволяет это ослабить (для некоррелированных алгоритмов диперсия композиции оказывается в $N$ раз меньше разброса отдельных алгоритмов). Если $N$ достаточно велико, то последующие увеличение числа `n_estimators` не позволят улучшить качество модели.\n",
    "\n",
    "В **градиентном бустинге** при достаточно большом $N$ появляется риск переобучения, так как увеличение числа базовых алгоритмов `n_estimators` будет продолжать настраиваться на обучающую выборку, уменьшая ошибку на ней, в то же время уменьшая обобщающую способность итоговой модели.\n",
    "\n",
    "Нарисуем графики качества в зависимости от числа деревьев для случайного леса на тренировочной и тестовой выборках. Также, нарисуем график времени обучения алгоритма от числа деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def get_model_scores(selected_model, n_list=np.arange(10, 250, 20), learning_rate=0.1):\n",
    "    \n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "    time_list = []\n",
    "\n",
    "    n_list = n_list\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\n",
    "    for n in tqdm(n_list):\n",
    "        model = selected_model(n_estimators=n, random_state=42)\n",
    "        # у RF нет параметра learning_rate\n",
    "        if type(model) == type(GradientBoostingClassifier()):\n",
    "            model.learning_rate = learning_rate \n",
    "        t_start = time()\n",
    "        model.fit(X_train, y_train)\n",
    "        t_end = time()\n",
    "        time_list.append(t_end - t_start)\n",
    "\n",
    "        preds_train = model.predict(X_train)\n",
    "        preds_test = model.predict(X_test)\n",
    "        acc_train.append(accuracy_score(y_train, preds_train))\n",
    "        acc_test.append(accuracy_score(y_test, preds_test))\n",
    "        \n",
    "    return acc_train, acc_test, time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_test, time_list = get_model_scores(RandomForestClassifier)\n",
    "\n",
    "params = {1: [acc_train, 'Random Forest', 'Train'], \n",
    "          2: [acc_test, 'Random Forest', 'Test'], \n",
    "          3: [time_list, \"Random Forest training time\", 'Time']}\n",
    "\n",
    "def plot_graphs(params):\n",
    "    figure(figsize=(15, 5))\n",
    "    color = 'b'\n",
    "    n_list = np.arange(10, 250, 20)\n",
    "    for num in range(1, 4):\n",
    "        plt.subplot(1, 3, num)\n",
    "        if num == 3:\n",
    "            color = 'r'\n",
    "        plot(n_list, params[num][0], label=params[num][1], c=color)\n",
    "        plt.legend()\n",
    "        plt.title(params[num][2])\n",
    "\n",
    "plot_graphs(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим, что с качество случайного леса растет и на обучающей, и на тестовой выборке, но если на обучающей мы довольно быстро достигаем числа деревьев, после которого качество уже не растет, то на тестовых данных видим более интересную, \"зубчатую\" картинку. Время обучения с увеличением числа деревьев тоже ожидаемо увеличивается. \n",
    "\n",
    "Теперь посмотрим, как меняется качество бустинга при увеличении числа деревьев.\n",
    "\n",
    "Постройте аналогичные графики, но для градиентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_test, time_list = get_model_scores(GradientBoostingClassifier)\n",
    "params = {1: [acc_train, 'Gradient boosting', 'Train'], \n",
    "          2: [acc_test, 'Gradient boosting', 'Test'], \n",
    "          3: [time_list, \"GB training time\", 'Time']}\n",
    "\n",
    "plot_graphs(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что у градиентный бустинг переобучился: его ошибка на обучающей выборке постоянно уменьшается, а ошибка на тестовых данных увеличивается, начиная с некоторого числа деревьев (так как на графике показано убывание точности (accuracy), равносильно сказать, что качество на тестовых данных падает). На train качество растет, на test падает - перед нами типичный случай переобучения. \n",
    "\n",
    "Значит, надо остановиться на некотором числе деревьев, с наименьшей ошибкой на графике на тестовых данных.\n",
    "\n",
    "Также, бороться с переобучением можно с помощью выбора очень простого базового алгоритма или\n",
    "снижением веса новых алгоритмов при помощи шага $\\eta$:\n",
    "$$a_N(x) = \\sum_{n=0}^N \\eta b_n(x).$$\n",
    "\n",
    "$\\eta$ - это наш learning_rate. Его уменьшение замедлит обучение по сравнению с бэггингом, но зато позволит побороть переобучение, хотя не до конца - оно останется при увеличении количества базовых алгоритмов для фиксированного $\\eta$.\n",
    "\n",
    "Давайте поменяем значение с 0.1 на 0.01 и обучим градиентный бустинг еще раз:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_test, time_list = get_model_scores(GradientBoostingClassifier, learning_rate=0.01)\n",
    "params = {1: [acc_train, 'Gradient boosting', 'Train'], \n",
    "          2: [acc_test, 'Gradient boosting', 'Test'], \n",
    "          3: [time_list, \"GB training time\", 'Time']}\n",
    "\n",
    "plot_graphs(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем улучшить качество, вспомним, что можно подбирать наилучшие гиперпараметры у модели через `GridSearchCV`. Подберите `max_features`, `max_depth` и `min_samples_leaf` для градиентного бустинга с его помощью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дефолтные параметры c accuracy 0.87 у нас были такие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(\n",
    "    criterion='mse',\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.max_features, model.max_depth, model.min_samples_leaf, model.n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "parameters = {'max_features': [3, 5, 10, None], \n",
    "              'max_depth': np.arange(1, 10, 2), \n",
    "              'min_samples_leaf': np.arange(1, 20, 5)}\n",
    "\n",
    "model = model = GradientBoostingClassifier(criterion='mse', n_estimators=10, random_state=42)\n",
    "clf = GridSearchCV(model, parameters)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(f'Параметры лучшей модели по качеству на test: {clf.best_params_}')\n",
    "print(f'Лучшая модель: {clf.best_estimator_}')\n",
    "print(f'Лучшая Accuracy: {round(clf.best_score_, 2)}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take away:\n",
    "\n",
    "Случайный лес и градиентный бустинг - это мощные ансамблевые алгоритмы, которые при должной настройке параметров показывают хорошее качество работы. Но надо помнить, что градиентный бустинг имеет склонность к переобучению с увеличением числа деревьев, и аккуратно подбирать число деревьев для алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дополнительные материалы для искушенных:\n",
    "    \n",
    "1. [Документация sklearn по GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "2. [Открытый курс машинного обучения. Тема 10. Градиентный бустинг](https://habr.com/ru/company/ods/blog/327250/)\n",
    "3. [Пишем XGBoost с нуля. Часть 1](https://habr.com/ru/company/vk/blog/438560/)\n",
    "4. [Пишем XGBoost с нуля. Часть 2](https://habr.com/ru/company/vk/blog/438562/)\n",
    "5. [Gradient Boosting explained](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные материалы:\n",
    "\n",
    "1) [Блог Александра Дьяконова. Анализ малых данных. Смещение (bias) и разброс (variance)](https://dyakonov.org/2018/04/25/%D1%81%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D0%B5-bias-%D0%B8-%D1%80%D0%B0%D0%B7%D0%B1%D1%80%D0%BE%D1%81-variance-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82/comment-page-1/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
