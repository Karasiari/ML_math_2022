{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cccb3519",
   "metadata": {},
   "source": [
    "## Градиентный бустинг. Часть 2: Catboost vs. LightGBM vs. XGBoost\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Murcha1990/ML_math_2022/blob/main/Семинары/sem12-bonus.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce0426",
   "metadata": {},
   "source": [
    "В прошлый раз мы посмотрели простую версию градиентного бустинга из scikit-learn, [придуманную в 1999 году Фридманом](https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451). Прогресс не стоит на месте, и на сегодняшний день есть три популярные библиотеки с разными имплементациями градиентного бустинга, которые на практике показывают лушие результаты: \n",
    "*  **XGBoost**. Появилась в 2014 году, [статья автора](https://mran.microsoft.com/snapshot/2020-07-15/web/packages/xgboost/vignettes/xgboost.pdf) вышла в 2016. После выхода быстро набрала популярность и оставалась стандартом до конца 2016 года. Об особенностях данной библиотеки рассказывалось на лекции.\n",
    "* **CatBoost** от компании Яндекс с релизом в 2017 году. Алгоритм можно запускать с дефолтными гиперпараметрами, потому тчо он является менее чувствительным к выбору их конкретных значений. Отлично умеет работать с категориальным признаками, при этом автоматически обрабатывая полученные на вход непредобработанные фичи. \n",
    "* **LightGBM**. Релиз в один год с Catboost, библиотека от Microsoft. Отличается очень быстрым построением композиции. Например, при построении узла дерева, вместо перебора по всем значениям признака, производится перебор значений гистограммы этого признака. Таким образом, вместо $O(N)$ требуется $O$(m), где $m$ - число бинов гистограммы. В отличие от других библиотек, строит деревья в глубину, при этом на каждом шаге строит вершину, дающую наибольшее уменьшение функционала."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e3129ca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|Критерий|Xgboost|Catboost|Lightgbm|\n",
    "|--|--|--|--|\n",
    "|Год релиза|2014|2017|2017|\n",
    "|Построение деревьев|симметрично по уровням|в глубину|асимметрично по уровням до максимальной глубины с прунингом|\n",
    "|Параметры контроля переобучения|learning_rate, depth, l2-leaf-reg (аналога min_child_weigth нет) |learning_rate, max_depth, num_leaves, min_data_in_leaf|learning_rate (eta), min_child_weigth, max_depth|\n",
    "|Контроль скорости обучения|rsm, iterations|feature_fraction, bagging_fraction, num_iterations|n_estimator, colsample_bytree, subsample|\n",
    "|Параметры категориальных фичей|cat_features, one_hot_max_size|categorical_feature|не доступно|\n",
    "|Бинаризация признаков|сетка выбирается заранее|-|перебор всех границ, выбор сетки на каждой итерации|\n",
    "|Скор сплита|Похожесть векторов градиентов |-| Смотрим на изменение функции ошибки|\n",
    "|Bootstrap|Можно перевзвешивать и менять интенсивность |-|-|\n",
    "|Рандомизация скора сплита|+ |-|-|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f364f9",
   "metadata": {},
   "source": [
    "### Основные параметры\n",
    "\n",
    "* objective – функция ошибки для настройки композиция\n",
    "* learning_rate / eta – скорость обучения\n",
    "* n_estimators / num_iterations – число итераций градиентного бустинга\n",
    "\n",
    "### Настройка сложности деревьев\n",
    "\n",
    "* max_depth – максимальная глубина \n",
    "* max_leaves / num_leaves – максимальное число вершин в дереве\n",
    "* gamma / min_gain_to_split – порог на уменьшение функции ошибки при расщеплении в дереве\n",
    "* min_data_in_leaf – минимальное число объектов в листе\n",
    "* min_sum_hessian_in_leaf – минимальная сумма весов объектов в листе, минимальное число объектов, при котором делается расщепление \n",
    "* lambda – коэффициент регуляризации (L2)\n",
    "* subsample / bagging_fraction – какую часть объектов обучения использовать для построения одного дерева \n",
    "* colsample_bytree / feature_fraction – какую часть признаков использовать для построения одного дерева \n",
    "\n",
    "Начать настройку можно с самых главных параметров: learning_rate и n_estimators. Один из них фиксируем, оставшийся из этих двух параметров подбираем (например, подбираем скорость обучения при n_estimators=100). Следующий параметр по важности - max_depth, так как мы хотим неглубокие деревья (в Catboost и LightGBM) для снижения переобучения.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "236227fc",
   "metadata": {},
   "source": [
    "**Техническое отступление**\n",
    "\n",
    "Данные библиотеки необходимо сначала устанавливать (можно через pip / conda или brew, если Вы работаете на MAC OS).\n",
    "Чтобы у Вас точно вопроизводился ноутбук и не было проблем из-за несовпадающих версий библиотек, рекомендуется через python создавать виртуальную среду."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7937f74b",
   "metadata": {},
   "source": [
    "В текущем ноутбуке использовались следующие версии библиотек: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb04ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import catboost\n",
    "import lightgbm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1becb5a1",
   "metadata": {},
   "source": [
    "### Catboost\n",
    "\n",
    "В алгоритме сделаны улучшения и выбор разных опций для борьбы с переобучением, подсчету сркднего таргета на отложенной выборке, подсчету статистик по категориальным фичам, бинаризацией фичей, рандомизации скора сплита, разные типы бутсрапирования. \n",
    "\n",
    "Давайте сначала зафиксируем все гиперпараметры со значениями по умолчанию, кроме количества деревьев в композиции - `n_estimators`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16301cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surface(X, y, clf):\n",
    "    h = 0.2\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Добавим на график сами наблюдения\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_repeated=0,\n",
    "                           n_classes=2, n_clusters_per_class=2,\n",
    "                           flip_y=0.05, class_sep=0.8, random_state=241)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier \n",
    "\n",
    "catboost = CatBoostClassifier(n_estimators=300, logging_level='Silent')\n",
    "catboost.fit(X_train, y_train)\n",
    "plot_surface(X_test, y_test, catboost)\n",
    "\n",
    "print(roc_auc_score(y_test, catboost.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = [1, 5, 10, 100, 200, 300, 400, 500, 600, 700]\n",
    "quals_train = []\n",
    "quals_test = []\n",
    "for n in n_trees:\n",
    "    catboost = CatBoostClassifier(iterations=n, logging_level='Silent')\n",
    "    catboost.fit(X_train, y_train)\n",
    "    q_train = roc_auc_score(y_train, catboost.predict_proba(X_train)[:, 1])\n",
    "    q_test = roc_auc_score(y_test, catboost.predict_proba(X_test)[:, 1])\n",
    "    quals_train.append(q_train)\n",
    "    quals_test.append(q_test)\n",
    "    \n",
    "plt.plot(n_trees, quals_train, marker='o', label='train')\n",
    "plt.plot(n_trees, quals_test, marker='o', label='test')\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36e153",
   "metadata": {},
   "source": [
    "## Xgboost\n",
    "\n",
    "1. Базовый алгоритм приближает направление, посчитанное с учетом второй производной функции потерь\n",
    "\n",
    "2. Функционал регуляризуется – добавляются штрафы за количество листьев и за норму коэффициентов\n",
    "\n",
    "3. При построении дерева используется критерий информативности, зависящий от оптимального вектора сдвига\n",
    "\n",
    "4. Критерий останова при обучении дерева также зависит от оптимального сдвига\n",
    "\n",
    "Ссылка на [источник](https://github.com/esokolov/ml-course-hse/blob/master/2021-fall/lecture-notes/lecture11-ensembles.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92bdf78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=300, verbosity=0)\n",
    "xgb.fit(X_train, y_train)\n",
    "plot_surface(X_test, y_test, xgb)\n",
    "\n",
    "print(roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a290d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = [1, 5, 10, 100, 200, 300, 400, 500, 600, 700]\n",
    "quals_train = []\n",
    "quals_test = []\n",
    "for n in n_trees:\n",
    "    xgboost = XGBClassifier(n_estimators=n, verbosity=0)\n",
    "    xgboost.fit(X_train, y_train)\n",
    "    q_train = roc_auc_score(y_train, xgboost.predict_proba(X_train)[:, 1])\n",
    "    q_test = roc_auc_score(y_test, xgboost.predict_proba(X_test)[:, 1])\n",
    "    quals_train.append(q_train)\n",
    "    quals_test.append(q_test)\n",
    "    \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_trees, quals_train, marker='.', label='train')\n",
    "plt.plot(n_trees, quals_test, marker='.', label='test')\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14848b",
   "metadata": {},
   "source": [
    "Видно, что переобучились - качество на тесте только падает."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973e678",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lightgbm = LGBMClassifier(n_estimators=300)\n",
    "lightgbm.fit(X_train, y_train)\n",
    "plot_surface(X_test, y_test, lightgbm)\n",
    "\n",
    "print(roc_auc_score(y_test, lightgbm.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = [1, 5, 10, 100, 200, 300, 400, 500, 600, 700]\n",
    "quals_train = []\n",
    "quals_test = []\n",
    "for n in n_trees:\n",
    "    lightgbm = LGBMClassifier(n_estimators=n)\n",
    "    lightgbm.fit(X_train, y_train)\n",
    "    q_train = roc_auc_score(y_train, lightgbm.predict_proba(X_train)[:, 1])\n",
    "    q_test = roc_auc_score(y_test, lightgbm.predict_proba(X_test)[:, 1])\n",
    "    quals_train.append(q_train)\n",
    "    quals_test.append(q_test)\n",
    "    \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_trees, quals_train, marker='o', label='train')\n",
    "plt.plot(n_trees, quals_test, marker='o', label='test')\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148c39f",
   "metadata": {},
   "source": [
    "В целом, у LightGBM получилась та же проблема с переобучением, как у Xgboost. Нужно дальше подбирать гиперпараметры для этих двух."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa170d",
   "metadata": {},
   "source": [
    "Попробуем взять фиксированное количество деревьев `n_estimators`, но будем менять их максимальную глубину `max_depth`. У этих алгоритмов разное время обучения, поэтому возьмем какой-то небольшой диапазон глубины и сравним все три модели - Catboost, LightGBM, Xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811fdb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_diff_depths(model=LGBMClassifier, depth_range=list(range(1, 5)), n_trees=10):\n",
    "    roc_auc_train = []\n",
    "    roc_auc_test = []\n",
    "    for i in depth_range:\n",
    "        clf = model(n_estimators=n_trees, max_depth=i)\n",
    "        if type(clf) == type(CatBoostClassifier()):\n",
    "            clf = CatBoostClassifier(n_estimators=n_trees, max_depth=i, logging_level=\"Silent\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        q_train = roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1])\n",
    "        q_test = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "        roc_auc_train.append(q_train)\n",
    "        roc_auc_test.append(q_test)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(depth_range, roc_auc_train, marker='o', label='train')\n",
    "    plt.plot(depth_range, roc_auc_test, marker='o', label='test')\n",
    "    plt.title(f'{model}')\n",
    "    plt.xlabel('Depth')\n",
    "    plt.ylabel('AUC-ROC')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b61d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff_depths(model=LGBMClassifier, depth_range=list(range(1, 16, 2)), n_trees=100)\n",
    "plot_model_diff_depths(model=CatBoostClassifier, depth_range=list(range(1, 16, 2)), n_trees=100)\n",
    "plot_model_diff_depths(model=XGBClassifier, depth_range=list(range(1, 16, 2)), n_trees=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7272f497",
   "metadata": {},
   "source": [
    "Когда мы обучили лучшие версии моделей, можно их сохранить и использовать для получения предсказаний, например, на новом батче данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5970a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранить\n",
    "lightgbm.booster_.save_model('lightgbm.txt')\n",
    "catboost.save_model('catboost.cbm', format='cbm')\n",
    "xgboost.save_model('xgboost.json')\n",
    "\n",
    "# Загрузить\n",
    "lightgbm = LGBMClassifier(model_file='mode.txt')\n",
    "catboost = catboost.load_model('catboost.cbm')\n",
    "xgboost = xgboost.load_model('xgboost.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c4337",
   "metadata": {},
   "source": [
    "### Блендинг"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2094c5c9",
   "metadata": {},
   "source": [
    "В этом подходе предсказания строятся как взвешенная сумма базовых алгоритмов. \n",
    "\n",
    "Рассмотрим простой пример блендинга градиентного бустинга и линейной регрессии для датасете `fetch_california_housing` и оценкой качества через RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58dbb3",
   "metadata": {},
   "source": [
    "Поделим выборку на обучающую (60%), тестовую (20%) и валидационную (20%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X_init = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y_init = data.target\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X_init, y_init, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "assert X_init.shape[0] == X_train.shape[0] + X_val.shape[0] + X_test.shape[0]\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    error = (y_true - y_pred) ** 2\n",
    "    return np.sqrt(np.mean(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8f7c4",
   "metadata": {},
   "source": [
    "Посмотрим, какое у нас качество у алгоритмов, если просто обучим на train и проверим качество на test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "cbm = CatBoostRegressor(iterations=100, max_depth=4, learning_rate=0.01, loss_function='RMSE', logging_level='Silent')\n",
    "cbm.fit(X_train, y_train)\n",
    "test_pred_cbm = cbm.predict(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "test_pred_lr = lr.predict(X_test)\n",
    "\n",
    "print(\"Test RMSE Linear Regression = %.3f\" % rmse(y_test, test_pred_lr))\n",
    "print(\"Test RMSE Catboost = %.3f\" % rmse(y_test, test_pred_cbm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ce0cf",
   "metadata": {},
   "source": [
    "Представим новый алгоритм $a(x)$ как взвешенную сумму из базовых алгоритмов:\n",
    "$$\n",
    "    a(x)\n",
    "    =\n",
    "    \\sum_{n = 1}^{N}\n",
    "    w_n b_n(x),\n",
    "$$\n",
    "где $\\sum_{n} w_n =1$ и нам нужно подобрать $w_n$. \n",
    "\n",
    "Сначала рассмотрим более простой случай, когда пробуем подбирать, с какими весами нам взять предсказания алгоритмов методом перебора пар весов (т.к. у нас только два алгоритма). \n",
    "\n",
    "Будем веса подбирать на валидации, а проверять качество на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c47e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_weights(y_true, y_pred_1, y_pred_2):\n",
    "    grid = np.linspace(0, 1, 1000)\n",
    "    metric = []\n",
    "    for w_0 in grid:\n",
    "        w_1 = 1 - w_0\n",
    "        y_a = w_0 * y_pred_1 + w_1 * y_pred_2\n",
    "        metric.append([rmse(y_true, y_a), w_0, w_1])\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41aa170",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_cbm = cbm.predict(X_val)\n",
    "val_pred_lr = lr.predict(X_val)\n",
    "\n",
    "rmse_blending_train, w_0, w_1 = min(select_weights(y_val, val_pred_cbm, val_pred_lr), key=lambda x: x[0])\n",
    "rmse_blending_train, w_0, w_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28629f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(rmse(y_test, test_pred_cbm * w_0 +  test_pred_lr * w_1), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624fbd0e",
   "metadata": {},
   "source": [
    "В данном случае видно, что нам с помощью блендинга с весами примерно 26% из градиентного бустинга и 74% из линейной регрессии удалось снизить ошибку на тесте до 4.63. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d5996",
   "metadata": {},
   "source": [
    "Давайте теперь напишем классическую версию блендинга, который выполняется по следующей схеме. Возьмем обучающую и тестовую выборку и разделим обучающую выборку на две части. На первой части обучим базовые алгоритмы, на второй - обучим мета-алгоритм из предсказаний базовых алгоритмов, и потом получим предсказания тестовых мета-признаках. \n",
    "<img src='https://alexanderdyakonov.files.wordpress.com/2017/03/stacking.png?w=1400' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff547f31",
   "metadata": {},
   "source": [
    "Посмотрим на все шаги с самого начала, c момента загрузки исходных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "X_init = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y_init = data.target\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X_init, y_init, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "assert X_init.shape[0] == X_train.shape[0] + X_val.shape[0] + X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d423ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "gb = CatBoostRegressor(iterations=100, max_depth=4, learning_rate=0.01, loss_function='RMSE', logging_level='Silent')\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "meta_train_df = pd.DataFrame()\n",
    "meta_train_df['gb_preds'] = gb.predict(X_val)\n",
    "meta_train_df['lr_preds'] = lr.predict(X_val)\n",
    "\n",
    "meta_algo = LGBMRegressor()\n",
    "meta_algo.fit(meta_train_df, y_val)\n",
    "\n",
    "meta_pred_df = pd.DataFrame()\n",
    "meta_pred_df['gb_preds'] = gb.predict(X_test)\n",
    "meta_pred_df['lr_preds'] = lr.predict(X_test)\n",
    "test_preds_meta = meta_algo.predict(meta_pred_df)\n",
    "\n",
    "rmse(y_test, test_preds_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7583f",
   "metadata": {},
   "source": [
    "Получается, что при блендинге базовые алгоритмы и мета-алгоритм не используют весь объем выборки обучения, что является недостатком. Для повышения качества нужно усреднять несколько блендигов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d526a",
   "metadata": {},
   "source": [
    "### Стэкинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4491b6a7",
   "metadata": {},
   "source": [
    "Попробуем реализовать стэкинг. Выборку разбивают на два фолда, последовательно перебирая фолды, обучают базовые алгоритмы на всех фолдах, кроме одного, а на оставшемся получают ответы базовых алгоритмов и используют их как значения соответствующих признаков на этом фолде. Для получения мета-признаков объектов тестовой выборки базовые алгоритмы обучают на всей обучающей выборке и берут их ответы на тестовой.\n",
    "\n",
    "<img src='https://alexanderdyakonov.files.wordpress.com/2017/03/stacking-2b.png?w=1400' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea34549",
   "metadata": {},
   "source": [
    "Для стэкинга можно пользоваться встроенной имплементацией в sklearn. Возьмем случайный лес и линейную регрессию как базовые алгоритмы, и потом обучим поверх с помощью 10-фолдовой кросс-валидации мета-алгоритм - градиентный бустинг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
    "\n",
    "estimators = [('rf', RandomForestRegressor(n_estimators=200,random_state=42)),\n",
    "              ('lr', LinearRegression())]\n",
    "\n",
    "reg = StackingRegressor(estimators=estimators,\n",
    "                        cv=10,\n",
    "                        final_estimator=CatBoostRegressor(iterations=700, max_depth=5, learning_rate=0.01, \n",
    "                                       loss_function='RMSE', logging_level='Silent'))\n",
    "\n",
    "reg.fit(X_train, y_train).score(X_test, y_test)\n",
    "reg_preds = reg.predict(X_test)\n",
    "round(rmse(y_test, reg_preds), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ab270",
   "metadata": {},
   "source": [
    "Кстати, довольно полезно, что после обучения градиентного бустинга можно посмотреть на то, какие из признаков оказались наиболее важные и значимые (feature importances). Рассмотрим на примере Catboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd952139",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = CatBoostRegressor(n_estimators=100, logging_level=\"Silent\")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "for val, name in sorted(zip(gb.feature_importances_, data.feature_names))[::-1]:\n",
    "    print(name, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a3603",
   "metadata": {},
   "source": [
    "Важность признака &mdash; это то, насколько в среднем меняется ответ модели при изменении значения данного признака (изменении значения разбиения).\n",
    "\n",
    "$$feature\\_importance_{F} = \\sum_{tree, leaves_F} (v_1 - avr)^2\\cdot c_1 +(v_2 - avr)^2\\cdot c_2\\\\\n",
    "\\qquad avr = \\frac{v_1 \\cdot c_1 + v_2 \\cdot c_2}{c_1 + c_2}.$$\n",
    "\n",
    "Запись $leaves_F$ означает разбиение, зависящее от признака $F$.\n",
    "\n",
    "Мы сравниваем поддеревья, отличающиеся значением сплита в узле на пути к ним: если условие сплита выполняется, объект попадает в левое поддерево, иначе &mdash; в правое. $c_1, c_2 - $  число объектов обучающего датасета, попавших в левое и правое поддерево соотвественно, либо суммарный вес этих объектов, если используются веса; $v1, v2 -$значение модели в левом и правом поддереве.\n",
    "Значения $feature\\_importance$ нормируются и суммируются в 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c602f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(dict(zip(data.feature_names, gb.feature_importances_)), \n",
    "                       orient='index').reset_index()\n",
    "df.columns = ['features', 'score']\n",
    "df.sort_values(by='score', ascending=False, inplace=True)\n",
    "df.plot(x='features', y='score', kind='bar', title='Feature importances')\n",
    "plt.ylabel('Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2482bbc",
   "metadata": {},
   "source": [
    "Так же, для измерения важности признаков используются вектора Шепли. Имплементация доступна в [библиотеке SHAP](https://github.com/slundberg/shap). Будем зашумлять входные данные по каждой переменной, чтобы понять, важная она или нет. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962e7af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install shap==0.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "import shap\n",
    "\n",
    "shap_values = gb.get_feature_importance(Pool(X), type='ShapValues')\n",
    "\n",
    "expected_value = shap_values[0,-1]\n",
    "shap_values = shap_values[:,:-1]\n",
    "\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Диапазон значений целевой переменной ({y.min()}, {y.max()})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0465a4",
   "metadata": {},
   "source": [
    "Как его читать:\n",
    "\n",
    "- Значения по оси X - отрицательное или положительное влияние на целевую переменную при изменении признаков. \n",
    "- Чем краснее точки на графике, тем выше значения фичи в ней\n",
    "- Чем толще линия на графике, тем больше таких точек наблюдения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52160f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
